# Answer Evaluator

Welcome to the **Answer Evaluator** repository! This project leverages advanced **Machine Learning (ML)** and **Natural Language Processing (NLP)** techniques to accurately evaluate student answers. Our system achieves an impressive accuracy rate of 89%.

## Project Overview

The **Answer Evaluator** uses a variety of NLP techniques to evaluate student responses against expected answers. The system calculates multiple scores based on different criteria and then combines these scores using a weighted average to provide a final evaluation score. The key features include:

- **Preprocessing Text**: Tokenization and Lemmatization
- **Exact and Partial Match**: Comparing the student\u2019s answer to the expected answer
- **Cosine Similarity**: Measuring similarity between texts
- **Sentiment Analysis**: Evaluating the sentiment of the response
- **Enhanced Sentence Match**: Using pre-trained models for semantic similarity
- **Multinomial Naive Bayes**: Probabilistic analysis
- **Coherence and Relevance Scores**: Assessing logical flow and content relevance

## Technologies Used

- **Flask**: Micro web framework for Python
- **Python**: Primary programming language
- **Jupyter Notebook**: Interactive computational environment
- **HTML, CSS, Bootstrap**: Frontend development
- **Gemini AI**: For advanced NLP models
- **Machine Learning & NLP**: Core of the evaluation system
- **SQL**: Database management
